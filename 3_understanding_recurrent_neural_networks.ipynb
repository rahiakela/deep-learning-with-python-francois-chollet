{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3-understanding-recurrent-neural-networks.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMnQ7+RS50qbDt1j9958X1m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/deep-learning-with-python-francois-chollet/blob/6-deep-learning-for-text-and-sequences/3_understanding_recurrent_neural_networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEmGMZ4jIJgm",
        "colab_type": "text"
      },
      "source": [
        "# Understanding recurrent neural networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_FHcZb4ILRf",
        "colab_type": "text"
      },
      "source": [
        "**A major characteristic of all neural networks you’ve seen so far, such as densely connected networks and convnets, is that they have no memory. Each input shown to them is processed independently, with no state kept in between inputs.**\n",
        "\n",
        "**With such networks, in order to process a sequence or a temporal series of data points, you have to show the entire sequence to the network at once: turn it into a single data point.**\n",
        "\n",
        "For instance, this is what you did in the IMDB example: an entire movie review was transformed into a single large vector(flatten) and processed in one go. Such networks are called **feedforward networks**.\n",
        "\n",
        "In contrast, as you’re reading the present sentence, you’re processing it word by word—or rather, eye saccade by eye saccade—while keeping memories of what came before; this gives you a fluid representation of the meaning conveyed by this sentence.\n",
        "\n",
        "**Biological intelligence processes information incrementally while maintaining an internal model of what it’s processing, built from past information and constantly updated as new information comes in**.\n",
        "\n",
        "A **recurrent neural network (RNN)** adopts the same principle, albeit in an extremely simplified version: **it processes sequences by iterating through the sequence elements and maintaining a state containing information relative to what it has seen so far. In effect, an RNN is a type of neural network that has an internal loop.**\n",
        "\n",
        "The state of the RNN is reset between processing two different,\n",
        "independent sequences (such as two different IMDB reviews), so you still consider one sequence a single data point: **a single input to the network. What changes is that this data point is no longer processed in a single step; rather, the network internally loops over sequence elements.**\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/deep-learning-with-python/recurrent-neural-network.png?raw=1' width='800'/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-cxGszvOwJe",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIw5Nx0tOx6C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense, LSTM\n",
        "\n",
        "from tensorflow.keras.datasets import imdb\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import string\n",
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kv4KQLrDO-JQ",
        "colab_type": "text"
      },
      "source": [
        "## Numpy implementation of a simple RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QAyWdz2O_Ov",
        "colab_type": "text"
      },
      "source": [
        "To make these notions of loop and state clear, let’s implement the forward pass of a toy RNN in Numpy. This RNN takes as input a sequence of vectors, which you’ll encode as a 2D tensor of size (timesteps, input_features). It loops over timesteps, and at each timestep, it considers its current state at t and the input at t (of shape (input_features,), and combines them to obtain the output at t. You’ll then set the state for the next step to be this previous output. \n",
        "\n",
        "For the first timestep, the previous output isn’t defined; hence, there is no current state. So, you’ll initialize the state as an allzero vector called the initial state of the network.\n",
        "\n",
        "In pseudocode, this is the RNN.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XabYv-S9Pwgs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "state_t = 0                       # the state at t\n",
        "for input_t in input_sequence:    # iterates over sequence elements\n",
        "  output_t = f(input_t, state_t)\n",
        "  state_t = output_t              # the previous output becomes the state for the next iteration."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQ4A_GkxQYIz",
        "colab_type": "text"
      },
      "source": [
        "You can even flesh out the function f: the transformation of the input and state into an output will be parameterized by two matrices, W and U, and a bias vector. It’s similar to the transformation operated by a densely connected layer in a feedforward network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjk_x4sVQrO2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "state_t = 0                       # the state at t\n",
        "for input_t in input_sequence:    # iterates over sequence elements\n",
        "  output_t = activation(dot(W, input_t) + dot(U, state_t) + b)\n",
        "  state_t = output_t              # the previous output becomes the state for the next iteration."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECYyXabyRNWZ",
        "colab_type": "text"
      },
      "source": [
        "To make these notions absolutely unambiguous, let’s write a naive Numpy implementation of the forward pass of the simple RNN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NHd3WTqRPSI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "timesteps = 100               # number of timesteps in the input sequence\n",
        "input_features = 32           # dimensionality of the input feature space\n",
        "output_features = 64          # dimensionality of the output feature space\n",
        "\n",
        "# input data: random noise for the sake of the example\n",
        "inputs = np.random.random((timesteps, input_features))\n",
        "\n",
        "# initial state: an all-zero vector\n",
        "state_t = np.zeros((output_features, ))\n",
        "\n",
        "# creates random weight matrices\n",
        "W = np.random.random((output_features, input_features))\n",
        "U = np.random.random((output_features, output_features))\n",
        "b = np.random.random((output_features, ))\n",
        "\n",
        "successive_outputs = []\n",
        "# input_t is a vector of shape (input_features,).\n",
        "for input_t in inputs:\n",
        "  # combines the input with the current state (the previous output) to obtain the current output\n",
        "  output_t = np.tanh(np.dot(W, input_t) + np.dot(U, state_t) + b)\n",
        "  successive_outputs.append(output_t)\n",
        "\n",
        "  # updates the state of the network for the next timestep\n",
        "  state_t = output_t\n",
        "\n",
        "# the final output is a 2D tensor of shape (timesteps, output_features).\n",
        "final_output_sequence = np.concatenate(successive_outputs, axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5fEJecfU-gH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "367253d2-0f2c-4ce7-9513-48b21df615dc"
      },
      "source": [
        "final_output_sequence.shape"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6400,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56rN68v4VL0r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "be0bdb09-de85-4965-be79-09e194e7a10e"
      },
      "source": [
        "final_output_sequence[:10]"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.99999926, 0.99999999, 0.99999763, 0.99999973, 0.99999797,\n",
              "       0.99999984, 0.9999999 , 0.99999997, 0.9999993 , 0.99999992])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziyZbduDVif-",
        "colab_type": "text"
      },
      "source": [
        "Easy enough: in summary, **an RNN is a for loop that reuses quantities computed during the previous iteration of the loop, nothing more.** \n",
        "\n",
        "Of course, there are many different RNNs fitting this definition that you could build—this example is one of the simplest RNN formulations. RNNs are characterized by their step function, such as the following function in this case.\n",
        "\n",
        "```python\n",
        "output_t = np.tanh(np.dot(W, input_t) + np.dot(U, state_t) + b)\n",
        "```\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/deep-learning-with-python/simple-RNN.png?raw=1' width='800'/>\n",
        "\n",
        "In this example, the final output is a 2D tensor of shape (timesteps,\n",
        "output_features), where each timestep is the output of the loop at time t.\n",
        "Each timestep t in the output tensor contains information about timesteps 0\n",
        "to t in the input sequence—about the entire past. \n",
        "\n",
        "For this reason, in many cases, you don’t need this full sequence of outputs; you just need the last output (output_t at the end of the loop), because it already contains information about the entire sequence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDgCqGi8J9pg",
        "colab_type": "text"
      },
      "source": [
        "## A recurrent layer in Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IISDfy5WOZ0",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}