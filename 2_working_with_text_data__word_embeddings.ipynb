{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2_working_with_text_data__word_embeddings.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/deep-learning-with-python-francois-chollet/blob/6-deep-learning-for-text-and-sequences/2_working_with_text_data__word_embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCLcK4vPkXXT",
        "colab_type": "text"
      },
      "source": [
        "# Working with text data: word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhQlXAUmkwsv",
        "colab_type": "text"
      },
      "source": [
        "Text is one of the most widespread forms of sequence data. It can be understood as either a sequence of characters or a sequence of words, but it’s most common to work at the level of words.\n",
        "\n",
        "The deep-learning sequence-processing models can use text to produce a basic form of natural-language understanding, sufficient for applications including:\n",
        "\n",
        "* document classification, \n",
        "* sentiment analysis, \n",
        "* author identification, \n",
        "* and even question-answering (QA)\n",
        "\n",
        "Of course, keep in mind that none of these deeplearning models truly understand text in a human sense; rather, these models can map the statistical structure of written language, which is sufficient to solve many simple textual tasks. \n",
        "\n",
        "Deep learning for natural-language processing is pattern recognition\n",
        "applied to words, sentences, and paragraphs, in much the same way that computer vision is pattern recognition applied to pixels.\n",
        "\n",
        "Like all other neural networks, deep-learning models don’t take as input raw text: they only work with numeric tensors. **Vectorizing text** is the process of transforming text into numeric tensors. \n",
        "\n",
        "This can be done in multiple ways:\n",
        "* Segment text into words, and transform each word into a vector.\n",
        "* Segment text into characters, and transform each character into a vector.\n",
        "* Extract n-grams of words or characters, and transform each n-gram into a vector.N-grams are overlapping groups of multiple consecutive words or characters.\n",
        "\n",
        "Collectively, the different units into which you can break down text (words, characters, or n-grams) are called tokens, and breaking text into such tokens is called tokenization. All text-vectorization processes consist of applying some tokenization scheme and then associating numeric vectors with the generated tokens. These vectors, packed into sequence tensors, are fed into deep neural networks. \n",
        "\n",
        "There are multiple ways to associate a vector with a token. I’ll present two major ones: \n",
        "\n",
        "* one-hot encoding of tokens, \n",
        "* and token embedding (typically used exclusively for words, and called\n",
        "word embedding).\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/deep-learning-with-python/text-to-tokens-to-vectors.png?raw=1' width='800'/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UX6W7ry3oZ0i",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-nh6BM2ocDO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense, Embedding\n",
        "\n",
        "from tensorflow.keras.datasets import imdb\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import string\n",
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Trg_rlvaogpI",
        "colab_type": "text"
      },
      "source": [
        "## Using word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAm_0dVAoi_m",
        "colab_type": "text"
      },
      "source": [
        "Another popular and powerful way to associate a vector with a word is the use of dense word vectors, also called word embeddings. Whereas the vectors obtained through one-hot encoding are binary, sparse (mostly made of zeros), and very high-dimensional (same dimensionality as the number of words in the vocabulary), word embeddings are lowdimensional floating-point vectors (that is, dense vectors, as opposed to sparse vectors).\n",
        "\n",
        "Unlike word vectors obtained via one-hot encoding, word embeddings are learned from data. It is common to see word embeddings that are 256-dimensional, 512-dimensional, or 1024-dimensional when dealing with very large vocabularies. On the other hand, one-hot encoding words generally leads to vectors that are 20,000-dimensional or higher (capturing a vocabulary of 20,000 token in this case). So, word embeddings pack more information into far fewer dimensions.\n",
        "\n",
        "<img src='https://s3.amazonaws.com/book.keras.io/img/ch6/word_embeddings.png?raw=1' width='800'/>\n",
        "\n",
        "There are two ways to obtain word embeddings:\n",
        "\n",
        "* Learn word embeddings jointly with the main task you care about (e.g. document classification or sentiment prediction). In this setup, you would start with random word vectors, then learn your word vectors in the same way that you learn the weights of a neural network.\n",
        "* Load into your model word embeddings that were pre-computed using a different machine learning task than the one you are trying to solve. These are called \"pre-trained word embeddings\".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiQA-DoAlC6R",
        "colab_type": "text"
      },
      "source": [
        "### Learning word embeddings with the Embedding layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLL705CqlGoc",
        "colab_type": "text"
      },
      "source": [
        "The simplest way to associate a dense vector to a word would be to pick the vector at random. The problem with this approach is that the resulting embedding space would have no structure: for instance, the words \"accurate\" and \"exact\" may end up with completely different embeddings, even though they are interchangeable in most sentences. It would be very difficult for a deep neural network to make sense of such a noisy, unstructured embedding space.\n",
        "\n",
        "To get a bit more abstract: the geometric relationships between word vectors should reflect the semantic relationships between these words. Word embeddings are meant to map human language into a geometric space. For instance, in a reasonable embedding space, we would expect synonyms to be embedded into similar word vectors, and in general we would expect the geometric distance (e.g. L2 distance) between any two word vectors to relate to the semantic distance of the associated words (words meaning very different things would be embedded to points far away from each other, while related words would be closer). Even beyond mere distance, we may want specific directions in the embedding space to be meaningful.\n",
        "\n",
        "It is thus reasonable to learn a new embedding space with every new task. Thankfully, backpropagation makes this really easy, and Keras makes it even easier. It's just about learning the weights of a layer: the Embedding layer.\n",
        "\n",
        "```python\n",
        "from keras.layers import Embedding\n",
        "\n",
        "# The Embedding layer takes at least two arguments:\n",
        "# the number of possible tokens, here 1000 (1 + maximum word index),\n",
        "# and the dimensionality of the embeddings, here 64.\n",
        "embedding_layer = Embedding(1000, 64)\n",
        "```\n",
        "\n",
        "The Embedding layer is best understood as a dictionary mapping integer indices (which stand for specific words) to dense vectors. It takes as input integers, it looks up these integers into an internal dictionary, and it returns the associated vectors. It's effectively a dictionary lookup.\n",
        "\n",
        "```python\n",
        "Word index >> Embedding layer >> Corresponding word vector\n",
        "```\n",
        "\n",
        "The Embedding layer takes as input a 2D tensor of integers, of shape (samples, sequence_length), where each entry is a sequence of integers. It can embed sequences of variable lengths, so for instance we could feed into our embedding layer above batches that could have shapes (32, 10) (batch of 32 sequences of length 10) or (64, 15) (batch of 64 sequences of length 15). All sequences in a batch must have the same length, though (since we need to pack them into a single tensor), so sequences that are shorter than others should be padded with zeros, and sequences that are longer should be truncated.\n",
        "\n",
        "This layer returns a 3D floating point tensor, of shape (samples, sequence_length, embedding_dimensionality). Such a 3D tensor can then be processed by a RNN layer or a 1D convolution layer.\n",
        "\n",
        "When you instantiate an Embedding layer, its weights (its internal dictionary of token vectors) are initially random, just like with any other layer. During training, these word vectors will be gradually adjusted via backpropagation, structuring the space into something that the downstream model can exploit. Once fully trained, your embedding space will show a lot of structure -- a kind of structure specialized for the specific problem you were training your model for.\n",
        "\n",
        "Let's apply this idea to the IMDB movie review sentiment prediction task that you are already familiar with. Let's quickly prepare the data. We will restrict the movie reviews to the top 10,000 most common words (like we did the first time we worked with this dataset), and cut the reviews after only 20 words. Our network will simply learn 8-dimensional embeddings for each of the 10,000 words, turn the input integer sequences (2D integer tensor) into embedded sequences (3D float tensor), flatten the tensor to 2D, and train a single Dense layer on top for classification.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQywk8Gmo1GT",
        "colab_type": "code",
        "outputId": "c60494ba-45be-40d6-f3ea-5bf8c87d643e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# Number of words to consider as features\n",
        "max_features = 10000\n",
        "\n",
        "# Cut texts after this number of words (among top max_features most common words)\n",
        "maxlen = 20\n",
        "\n",
        "# Load the data as lists of integers.\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "\n",
        "# turns our lists of integers into a 2D integer tensor of shape `(samples, maxlen)`\n",
        "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = pad_sequences(x_test, maxlen=maxlen)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItjbXps3pMgf",
        "colab_type": "code",
        "outputId": "c4ff102c-17fb-4a45-8923-6a614adceab7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        }
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "# specify the maximum input length to our Embedding layer so we can later flatten the embedded inputs\n",
        "model.add(Embedding(10000, 8, input_length=maxlen))\n",
        "# After the Embedding layer, our activations have shape `(samples, maxlen, 8)`.\n",
        "\n",
        "# flatten the 3D tensor of embeddings into a 2D tensor of shape `(samples, maxlen * 8)`\n",
        "model.add(Flatten())\n",
        "\n",
        "# We add the classifier on top\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 20, 8)             80000     \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 160)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1)                 161       \n",
            "=================================================================\n",
            "Total params: 80,161\n",
            "Trainable params: 80,161\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIwcfo3ytBNL",
        "colab_type": "code",
        "outputId": "2af3b393-3895-49f0-cbb9-7516fc65944d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        }
      },
      "source": [
        "history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.6701 - accuracy: 0.6220 - val_loss: 0.6212 - val_accuracy: 0.7000\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.5425 - accuracy: 0.7494 - val_loss: 0.5268 - val_accuracy: 0.7392\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.4615 - accuracy: 0.7857 - val_loss: 0.5006 - val_accuracy: 0.7512\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.4229 - accuracy: 0.8067 - val_loss: 0.4936 - val_accuracy: 0.7578\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.3973 - accuracy: 0.8236 - val_loss: 0.4945 - val_accuracy: 0.7602\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.3769 - accuracy: 0.8340 - val_loss: 0.4958 - val_accuracy: 0.7636\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.3573 - accuracy: 0.8473 - val_loss: 0.5002 - val_accuracy: 0.7608\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.3381 - accuracy: 0.8586 - val_loss: 0.5050 - val_accuracy: 0.7584\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.3192 - accuracy: 0.8684 - val_loss: 0.5111 - val_accuracy: 0.7528\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 2s 3ms/step - loss: 0.3011 - accuracy: 0.8777 - val_loss: 0.5178 - val_accuracy: 0.7508\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiKZaDi_rgKS",
        "colab_type": "text"
      },
      "source": [
        "We get to a validation accuracy of ~76%, which is pretty good considering that we only look at the first 20 words in every review. But note that merely flattening the embedded sequences and training a single Dense layer on top leads to a model that treats each word in the input sequence separately, without considering inter-word relationships and structure sentence (e.g. it would likely treat both \"this movie is shit\" and \"this movie is the shit\" as being negative \"reviews\"). It would be much better to add recurrent layers or 1D convolutional layers on top of the embedded sequences to learn features that take into account each sequence as a whole. That's what we will focus on in the next few sections."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8LjZEp0thyB",
        "colab_type": "text"
      },
      "source": [
        "### Using pre-trained word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RE7Hf7SwtES_",
        "colab_type": "text"
      },
      "source": [
        "Sometimes, you have so little training data available that could never use your data alone to learn an appropriate task-specific embedding of your vocabulary. What to do then?\n",
        "\n",
        "Instead of learning word embeddings jointly with the problem you want to solve, you could be loading embedding vectors from a pre-computed embedding space known to be highly structured and to exhibit useful properties -- that captures generic aspects of language structure. \n",
        "\n",
        "The rationale behind using pre-trained word embeddings in natural language processing is very much the same as for using pre-trained convnets in image classification: we don't have enough data available to learn truly powerful features on our own, but we expect the features that we need to be fairly generic, i.e. common visual features or semantic features. In this case it makes sense to reuse features learned on a different problem.\n",
        "\n",
        "Such word embeddings are generally computed using word occurrence statistics (observations about what words co-occur in sentences or documents), using a variety of techniques, some involving neural networks, others not. The idea of a dense, low-dimensional embedding space for words, computed in an unsupervised way, was initially explored by Bengio et al. in the early 2000s, but it only started really taking off in research and industry applications after the release of one of the most famous and successful word embedding scheme: the Word2Vec algorithm, developed by Mikolov at Google in 2013. Word2Vec dimensions capture specific semantic properties, e.g. gender.\n",
        "\n",
        "There are various pre-computed databases of word embeddings that can download and start using in a Keras Embedding layer. \n",
        "\n",
        "* Word2Vec is one of them. \n",
        "* Another popular one is called \"GloVe\", developed by Stanford researchers in 2014. It stands for \"Global Vectors for Word Representation\", and it is an embedding technique based on factorizing a matrix of word co-occurrence statistics. \n",
        "\n",
        "Let's take a look at how you can get started using GloVe embeddings in a Keras model. The same method will of course be valid for Word2Vec embeddings or any other word embedding database that you can download. We will also use this example to refresh the text tokenization techniques we introduced a few paragraphs ago: we will start from raw text, and work our way up."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t305iDvotfZ_",
        "colab_type": "text"
      },
      "source": [
        "## Putting it all together: from raw text to word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIJsyT-gthiS",
        "colab_type": "text"
      },
      "source": [
        "You’ll use a model similar to the one we just went over: \n",
        "\n",
        "* embedding sentences in sequences of vectors, \n",
        "* flattening them, \n",
        "* and training a Dense layer on top. \n",
        "\n",
        "But you’ll do so using pretrained word embeddings; and instead of using the pretokenized IMDB data packaged in Keras, you’ll start from scratch by downloading the original text data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RY_pLK6r_jtQ",
        "colab_type": "text"
      },
      "source": [
        "### Download the IMDB data as raw text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oofys_Mi66yS",
        "colab_type": "code",
        "outputId": "0583a980-68a9-4e35-d787-7dd1fe4578d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "# download IMDB dataset\n",
        "ds_path = keras.utils.get_file('aclImdb_v1.tar.gz', origin='http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz', extract=True)\n",
        "ds_path"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "84131840/84125825 [==============================] - 7s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/root/.keras/datasets/aclImdb_v1.tar.gz'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zr7DJM6n7StN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp /root/.keras/datasets/aclImdb_v1.tar.gz ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEJBPv-wxeFF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tarfile\n",
        "\n",
        "tar = tarfile.open('aclImdb_v1.tar.gz', \"r:gz\")\n",
        "tar.extractall()\n",
        "tar.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNtan1i6tRTW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "imdb_dir = 'aclImdb/'\n",
        "train_dir = os.path.join(imdb_dir, 'train')\n",
        "\n",
        "labels = []\n",
        "texts = []\n",
        "\n",
        "for label_type in ['neg', 'pos']:\n",
        "  dir_name = os.path.join(train_dir, label_type)\n",
        "  for fname in os.listdir(dir_name):\n",
        "    f = open(os.path.join(dir_name, fname))\n",
        "    texts.append(f.read())\n",
        "    f.close()\n",
        "    if label_type == 'neg':\n",
        "      labels.append(0)\n",
        "    else:\n",
        "      labels.append(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmNyQ2j9w0sl",
        "colab_type": "code",
        "outputId": "ea229c11-972d-42ca-ecae-8960533ac999",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        }
      },
      "source": [
        "texts[:5]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I saw this movie a few days ago and gamely jumped during the scary parts. I must admit, I found it pretty decent...until I started to THINK about what the characters were saying. Logical problems:<br /><br />1. Her boyfriend, who seems to be a pretty fit dude, makes no sound while being killed. Don\\'t you think that he might have at least tried to take the killer? <br /><br />2. When the remark is made that the gym teacher is \"SOOOO in love with Lisa,\" I almost screamed at the screen. When your best friend\\'s family HAS BEEN KILLED BY A TEACHER WHO WAS IN LOVE WITH HER, you don\\'t make comments like that if you have half of a heart.<br /><br />3. As soon as Nash asks the uncle how many exits they have in the house and the uncle catches on that there may be danger ahead, wouldn\\'t the smart thing to do be to get Donna, boyfriend, aunt, and uncle into a car and drive far, far away, then bait the house with the HRT and police force so that the killer has no way to get out?<br /><br />I could go on. And on. And on. Basically, the plot was decent, the characters weren\\'t profiled enough for you to actually feel any empathy when they were slaughtered and there were way too many errors.<br /><br />HOWEVER.<br /><br />This movie might be good for teenagers, or young couples just looking for a fun night out. If you don\\'t consider all the goofs, it\\'s a mediocre film.',\n",
              " 'I don\\'t see enough TV game shows to understand the attraction of SHOW ME THE MONEY, but I suppose it holds some appeal for undemanding audiences. Ostensibly a quiz show, it offers contestants huge sums of money for answering a few simple questions. However, its quiz elements play only a small part in the proceedings, which I find tortuously complicated. For example, before answering a question, a contestant selects which question is to be asked by choosing from among random \"A,\" \"B,\" or \"C\" choices. Does this serve any purpose other than to slow the game down? It would be a lot quicker simply to start with \"A.\" Contestants can pass on questions, but must answer one of the three questions in each category.<br /><br />After responding to a question, the contestant is then asked to \"lock in\" the answer--another delaying tactic. The contestant\\'s next task is to name which woman from about a dozen go-go dancers in cages is to unveil a card that indicates how much the question is worth. A correct answer adds the card\\'s dollar figure to the contestant\\'s running total; a wrong answer subtracts the same sum. This time-consuming step actually has some entertainment value, as it allows the audience to get a close look at the scantily clad and uniformly gorgeous dancers. Meanwhile, the contestant is reminded that an unlucky selection of the \"killer card\" will end the game instantly. This naturally makes the contestant sweat and causes further delays as the nervous contestant contemplates the sudden loss of the hundreds of thousands of dollars. My suspicion is that the possibility of sudden disaster is the show\\'s chief audience appeal.<br /><br />Meanwhile, the whole process is slowed down even more by a lot of empty banter between host William Shatner and the contestant, along with occasional routines by the caged dancers. All these delays burn up so much time that it might be possible for audiences to forget what the original question is by the time the correct answer is revealed.<br /><br />A typical 30-minute episode of JEOPARDY often gets through as many as 60 questions. The first 30 minutes of SMTM that I watched got through only six questions (many of which pertained to other TV shows). No one in his right mind would watch this show because it\\'s fun to play along by answering the questions at home. That leaves three possible reasons to watch the show.<br /><br />A. To see how a contestant responds to being on the verge of winning as much as one million dollars, only to lose everything in one stroke.<br /><br />B. To look at gorgeous young women performing sexually suggestive dance routines.<br /><br />C. To enjoy William Shatner\\'s scintillating banter.<br /><br />My choice is \"B,\" but the women aren\\'t on camera long enough to justify suffering through an hour of this show.',\n",
              " 'In 1967, mine workers find the remnants of an ancient vanished civilization named Abkani that believe there are the worlds of light and darkness. When they opened the gate between these worlds ten thousand years ago, something evil slipped through before the gate was closed. Twenty-two years ago, the Government Paranormal Research Agency Bureau 713 was directed by Professor Lionel Hudgens (Matthew Walker), who performed experiments with orphan children. On the present days, one of these children is the paranormal investigator Edward Carnby (Christian Slater), who has just gotten an Abkani artifact in South America, and is chased by a man with abilities. When an old friend of foster house disappears in the middle of the night, he discloses that demons are coming back to Earth. With the support of the anthropologist Aline Cedrac (Tara Reid) and the leader of the Bureau 713, Cmdr. Richard Burke (Stephen Dorff), and his squad, they battle against the evil creatures.<br /><br />In spite of having a charismatic good cast, leaded by Christian Slater, Tara Reid and Stephen Dorff, \"Alone in the Dark\" never works and is a complete mess, without development of characters or plot. The reason may be explained by the \"brilliant\" interview of director Uwe Boll in the Extras of the DVD, where he says that \"videogames are the bestsellers of the younger generations that are not driven by books anymore\". Further, his target audience would be people aged between twelve and twenty-five years old. Sorry, but I find both assertions disrespectful with the younger generations. I have a daughter and a son, and I know many of their friends and they are not that type of stupid stereotype the director says. Further, IMDb provides excellent statistics to show that Mr. Uwe Boll is absolutely wrong. My vote is three.<br /><br />Title (Brazil): \"Alone in the Dark \\x96 O Despertar do Mal\" (\"Alone in the Dark \\x96 The Awakening of the Evil\")',\n",
              " 'This was quite possibly the worst movie I have ever seen. I watched it with a large group of friends and after it was over not a one of us understood the plot. Aside from the lack of plot, the acting was atrocious, the \"special effects\" were not so special, and the writing was absolutely horrible. The movie\\'s only redeeming factor is that it\\'s so incredibly bad that it\\'s quite funny. You can\\'t help but laugh at a zombie being run over while actors are spewing crappy dialogue. I wouldn\\'t recommend this film to anyone looking for a good movie, but it\\'s something that a group of friends can get together and have a good laugh about. It\\'s now a running joke among my friends and I. 1 out of 10.',\n",
              " 'I think that my favorite part of this movie, the one that exemplifies the sheer pointless, stupidity and inanity of the proceedings, comes at the climax of the film. DOCTOR TED NELSON and his unmarried friend the Sheriff have finally cornered the Melting Man on a landing on some stairs in an electrical generating plant. Keep in mind that Nelson has been looking for the MM for nearly the entire film, and that the MM has killed and eaten several people at this point (including his boss), and Nelson is very aware that MM is violently insane and hungry for human flesh and blood.<br /><br />So the Sheriff has his gun pointed at MM, who is, and I give the movie and Rick Baker props for this, the most disgusting and terrifying object in human form that we have ever seen. And he yells a very important question to DOCTOR TED NELSON: \"WHAT DO WE DO NOW?!?!?\" <br /><br />The camera cuts over to DOCTOR TED NELSON, and it\\'s obvious that Ted has no idea what to do next. Apparently Ted was so intent on the problem of FINDING the Melting Man, he never thought to bring along some restraining devices, a lasso, or straitjacket, or a net, or some tranquilizer darts, or maybe a New Age tape by Vangelis to soothe the savage beast.<br /><br />So the sheriff panics and shoots, the Melting Man goes berserk, and hilarity ensues. <br /><br />Maybe this explains why NASA has been screwing around with the Space Shuttle program in sub-lunar space for the last 30 years instead of going back to the Moon or out to Mars like everyone knows they OUGHT to be doing. I dunno.<br /><br />Anyway, that\\'s the kind of lousy, lazy writing and direction that undercuts every aspect of this movie. It\\'s hard to say how good the actors actually are, because the movie has complete contempt for their characters.<br /><br />Two other incredibly painful sequences also ramp up the stupidity of the proceedings: There is a scene featuring the lumpiest old couple in the world trying to steal lemons from a grove, only to be torn apart by the Melting Man. This scene is a nadir in 70s cinema. I can guarantee you\\'ve never watched a more pointless and irritating setup with odder looking people in your entire life. And the Melting Man\\'s assault on the lady who lives in the house where they keep a horse who pees on the walls defies every attempt to process it.(BTW, I think famous film director Jonathon Demme has a walk-on in this scene as the redneck husband who goes in first to check on the house and never comes out again). The only thing that keeps the actress from literally chewing the scenery is that, as I said, their horse has apparently been peeing on it. And we are forced to watch her hysterics for at least two minutes longer than any SANE film director would hold the shot. <br /><br />Burr DeBenning ought to beat the crap out of IMM\\'s director and photographer. I remember him from an old Columbo episode where he looked MUCH better than he does here - no one\\'s idea of a leading man, but solid and unobtrusive. But no one could possibly be as unappealing in real life as his director makes him look here. <br /><br />Everyone else comes off a little better except for the old couple (and shut up, I know they were being played for laughs, but I ain\\'t laughing!) but not much. <br /><br />This definitely falls into the \\'So Bad You Can\\'t Look Away\\' category of cinema disasters. Still, I\\'d watch it again before I\\'d watch a lot of other 70\\'s and 80\\'s abortions ( \"Track of The Moonbeast\" and \"It Lives By Night\" come to mind), and MST\\'s coverage of it is great fun, so if you get a chance, watch the MST version.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frYlCiup10yN",
        "colab_type": "code",
        "outputId": "2cbd7cc7-01e4-4a15-ddf7-0a777b87af52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "labels[:5]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 0, 0, 0, 0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlG-80GT6E0w",
        "colab_type": "code",
        "outputId": "fa03fd12-6ddc-4635-e1ba-60c574a695cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "(len(texts), len(labels))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 25000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPlVTaND2QKW",
        "colab_type": "text"
      },
      "source": [
        "### Tokenize the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkOrM--02RFT",
        "colab_type": "text"
      },
      "source": [
        "Let's vectorize the texts we collected, and prepare a training and validation split. We will merely be using the concepts we introduced earlier in this section.\n",
        "\n",
        "Because pre-trained word embeddings are meant to be particularly useful on problems where little training data is available (otherwise, task-specific embeddings are likely to outperform them), we will add the following twist: we restrict the training data to its first 200 samples. So we will be learning to classify movie reviews after looking at just 200 examples..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kx_g9FA513a2",
        "colab_type": "code",
        "outputId": "9c214844-e2bd-4dd7-95d0-20dd4135f5fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "maxlen = 100                # We will cut reviews after 100 words\n",
        "training_samples = 200      # We will be training on 200 samples\n",
        "validation_samples = 10000  # We will be validating on 10000 samples\n",
        "max_words = 10000           # We will only consider the top 10,000 words in the dataset\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print(f'Found {str(len(word_index))} unique tokens.')\n",
        "\n",
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "labels = np.asarray(labels)\n",
        "print(f'Shape of data tensor: {data.shape}')\n",
        "print(f'Shape of label tensor: {labels.shape}')\n",
        "\n",
        "'''\n",
        "Split the data into a training set and a validation set\n",
        "But first, shuffle the data, since we started from data\n",
        "where sample are ordered (all negative first, then all positive).\n",
        "'''\n",
        "indices = np.arange(data.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "data = data[indices]\n",
        "labels = labels[indices]\n",
        "\n",
        "x_train = data[:training_samples]\n",
        "y_train = labels[:training_samples]\n",
        "x_val = data[training_samples: training_samples + validation_samples]\n",
        "y_val = labels[training_samples: training_samples + validation_samples]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 88582 unique tokens.\n",
            "Shape of data tensor: (25000, 100)\n",
            "Shape of label tensor: (25000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaktj7dT9MJe",
        "colab_type": "text"
      },
      "source": [
        "## Download the GloVe word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXZCxKVp9N2P",
        "colab_type": "text"
      },
      "source": [
        "Head to https://nlp.stanford.edu/projects/glove/ (where you can learn more about the GloVe algorithm), and download the pre-computed embeddings from 2014 English Wikipedia. It's a 822MB zip file named glove.6B.zip, containing 100-dimensional embedding vectors for 400,000 words (or non-word tokens). Un-zip it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQsDD0_H3Sth",
        "colab_type": "code",
        "outputId": "ca3a638f-71fb-4bb5-bf55-cabf8bc2c1e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "# download IMDB dataset\n",
        "glove_path = keras.utils.get_file('glove.6B.zip', origin='http://nlp.stanford.edu/data/glove.6B.zip', extract=True)\n",
        "glove_path"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://nlp.stanford.edu/data/glove.6B.zip\n",
            "862183424/862182613 [==============================] - 390s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/root/.keras/datasets/glove.6B.zip'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNeCFbNe-qIK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r /root/.keras/datasets/glove.6B.zip ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-6iAr6YBHIY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile('glove.6B.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('glove')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxv4PH9aBeS7",
        "colab_type": "text"
      },
      "source": [
        "### Pre-process the embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8P6MCAoBfgV",
        "colab_type": "text"
      },
      "source": [
        "Let's parse the un-zipped file (it's a txt file) to build an index mapping words (as strings) to their vector representation (as number vectors)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrOf_yaRBTlX",
        "colab_type": "code",
        "outputId": "19121c8a-a0f7-4edb-db4d-811d13960f00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "glove_dir = 'glove/'\n",
        "\n",
        "embeddings_index = {}\n",
        "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\n",
        "for line in f:\n",
        "  values = line.split()\n",
        "  word = values[0]\n",
        "  coefs = np.asarray(values[1:], dtype='float32')\n",
        "  embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print(f'Found {str(len(embeddings_index))} word vectors.')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjAxJAjmCo0d",
        "colab_type": "text"
      },
      "source": [
        "### Preparing the GloVe word-embeddings matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUelZzKSCput",
        "colab_type": "text"
      },
      "source": [
        "Now let's build an embedding matrix that we will be able to load into an Embedding layer. It must be a matrix of shape (max_words, \n",
        "embedding_dim), where each entry i contains the embedding_dim-dimensional vector for the word of index i in our reference word index (built during tokenization). Note that the index 0 is not supposed to stand for any word or token -- it's a placeholder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eREU0iWMCYjk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_dim = 100\n",
        "\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "  embedding_vector = embeddings_index.get(word)\n",
        "  if i < max_words:\n",
        "    if embedding_vector is not None:\n",
        "      # Words not found in embedding index will be all-zeros.\n",
        "      embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFj_FbtaDUjf",
        "colab_type": "text"
      },
      "source": [
        "## Define a model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSHX-RpFDXRj",
        "colab_type": "text"
      },
      "source": [
        "You’ll use the same model architecture as before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZUnAJykDTY0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "e10c36fa-e5e2-401a-e0e4-49ddd769487a"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.summary()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 100, 100)          1000000   \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 10000)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 32)                320032    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 1,320,065\n",
            "Trainable params: 1,320,065\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJQTP-6AHZxk",
        "colab_type": "text"
      },
      "source": [
        "### Load the GloVe embeddings in the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbmqQubYHdB5",
        "colab_type": "text"
      },
      "source": [
        "The Embedding layer has a single weight matrix: a 2D float matrix where each entry i is the word vector meant to be associated with index i. Simple enough. Let's just load the GloVe matrix we prepared into our Embedding layer, the first layer in our model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNQUvv7jHiLI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.layers[0].set_weights([embedding_matrix])\n",
        "model.layers[0].trainable = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOKoNjiuIMN2",
        "colab_type": "text"
      },
      "source": [
        "Additionally, we freeze the embedding layer (we set its trainable attribute to False), following the same rationale as what you are already familiar with in the context of pre-trained convnet features: when parts of a model are pre-trained (like our Embedding layer), and parts are randomly initialized (like our classifier), the pre-trained parts should not be updated during training to avoid forgetting what they already know. The large gradient update triggered by the randomly initialized layers would be very disruptive to the already learned features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSpD5b89IU9v",
        "colab_type": "text"
      },
      "source": [
        "### Train and evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0ovZNsFIXhg",
        "colab_type": "text"
      },
      "source": [
        "Let's compile our model and train it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aByQdYCZIb2Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "f918065c-116a-4c9e-e1c5-57d8eed68df5"
      },
      "source": [
        "model.compile(optimizer='RMSprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_val, y_val))\n",
        "model.save_weights('pre_trained_glove_model.h5')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "7/7 [==============================] - 1s 154ms/step - loss: 1.5567 - accuracy: 0.5150 - val_loss: 0.7849 - val_accuracy: 0.5047\n",
            "Epoch 2/10\n",
            "7/7 [==============================] - 1s 128ms/step - loss: 0.6059 - accuracy: 0.6550 - val_loss: 0.8547 - val_accuracy: 0.5047\n",
            "Epoch 3/10\n",
            "7/7 [==============================] - 1s 136ms/step - loss: 0.4956 - accuracy: 0.7750 - val_loss: 0.6833 - val_accuracy: 0.5698\n",
            "Epoch 4/10\n",
            "7/7 [==============================] - 1s 127ms/step - loss: 0.2952 - accuracy: 0.9250 - val_loss: 0.8442 - val_accuracy: 0.5221\n",
            "Epoch 5/10\n",
            "7/7 [==============================] - 1s 129ms/step - loss: 0.2802 - accuracy: 0.8950 - val_loss: 1.1398 - val_accuracy: 0.4988\n",
            "Epoch 6/10\n",
            "7/7 [==============================] - 1s 130ms/step - loss: 0.2606 - accuracy: 0.8900 - val_loss: 0.7497 - val_accuracy: 0.5543\n",
            "Epoch 7/10\n",
            "7/7 [==============================] - 1s 128ms/step - loss: 0.1115 - accuracy: 0.9950 - val_loss: 0.7532 - val_accuracy: 0.5735\n",
            "Epoch 8/10\n",
            "7/7 [==============================] - 1s 129ms/step - loss: 0.0574 - accuracy: 1.0000 - val_loss: 2.0511 - val_accuracy: 0.4962\n",
            "Epoch 9/10\n",
            "7/7 [==============================] - 1s 128ms/step - loss: 0.3529 - accuracy: 0.8450 - val_loss: 0.7622 - val_accuracy: 0.5801\n",
            "Epoch 10/10\n",
            "7/7 [==============================] - 1s 126ms/step - loss: 0.0303 - accuracy: 1.0000 - val_loss: 0.8911 - val_accuracy: 0.5550\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SiVgDlMJm_T",
        "colab_type": "text"
      },
      "source": [
        "Let's plot its performance over time:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UV30jCrUJj-E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "outputId": "17ae68b9-ecf6-4ed6-8d67-c08b8760326f"
      },
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9nQvcDOKF5p",
        "colab_type": "text"
      },
      "source": [
        "The model quickly starts overfitting, unsurprisingly given the small number of training samples. Validation accuracy has high variance for the same reason, but seems to reach high 50s.\n",
        "\n",
        "Note that your mileage may vary: since we have so few training samples, performance is heavily dependent on which exact 200 samples we picked, and we picked them at random. If it worked really poorly for you, try picking a different random set of 200 samples, just for the sake of the exercise (in real life you don't get to pick your training data).\n",
        "\n",
        "We can also try to train the same model without loading the pre-trained word embeddings and without freezing the embedding layer. In that case, we would be learning a task-specific embedding of our input tokens, which is generally more powerful than pre-trained word embeddings when lots of data is available. However, in our case, we have only 200 training samples. Let's try it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dC8AoHfGJwTJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "outputId": "78438472-ca1d-4869-998e-96eac14b8bc0"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer='RMSprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_val, y_val))\n",
        "model.save_weights('pre_trained_glove_model.h5')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 100, 100)          1000000   \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 10000)             0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 32)                320032    \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 1,320,065\n",
            "Trainable params: 1,320,065\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "7/7 [==============================] - 1s 148ms/step - loss: 0.7016 - accuracy: 0.4500 - val_loss: 0.6926 - val_accuracy: 0.5105\n",
            "Epoch 2/10\n",
            "7/7 [==============================] - 1s 136ms/step - loss: 0.5123 - accuracy: 0.9850 - val_loss: 0.6940 - val_accuracy: 0.5117\n",
            "Epoch 3/10\n",
            "7/7 [==============================] - 1s 133ms/step - loss: 0.3083 - accuracy: 0.9850 - val_loss: 0.6967 - val_accuracy: 0.5217\n",
            "Epoch 4/10\n",
            "7/7 [==============================] - 1s 137ms/step - loss: 0.1329 - accuracy: 1.0000 - val_loss: 0.7086 - val_accuracy: 0.5188\n",
            "Epoch 5/10\n",
            "7/7 [==============================] - 1s 133ms/step - loss: 0.0604 - accuracy: 1.0000 - val_loss: 0.7047 - val_accuracy: 0.5198\n",
            "Epoch 6/10\n",
            "7/7 [==============================] - 1s 166ms/step - loss: 0.0308 - accuracy: 1.0000 - val_loss: 0.7249 - val_accuracy: 0.5200\n",
            "Epoch 7/10\n",
            "7/7 [==============================] - 1s 136ms/step - loss: 0.0173 - accuracy: 1.0000 - val_loss: 0.7153 - val_accuracy: 0.5238\n",
            "Epoch 8/10\n",
            "7/7 [==============================] - 1s 134ms/step - loss: 0.0098 - accuracy: 1.0000 - val_loss: 0.7336 - val_accuracy: 0.5217\n",
            "Epoch 9/10\n",
            "7/7 [==============================] - 1s 134ms/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.7325 - val_accuracy: 0.5229\n",
            "Epoch 10/10\n",
            "7/7 [==============================] - 1s 133ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.7339 - val_accuracy: 0.5231\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NIJA23sKSyi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "outputId": "520821ba-1edc-455c-da2b-3e1a872c0ace"
      },
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AtgcZjXKsS6",
        "colab_type": "text"
      },
      "source": [
        "Validation accuracy stalls in the low 50s. So in our case, pre-trained word embeddings does outperform jointly learned embeddings. If you increase the number of training samples, this will quickly stop being the case -- try it as an exercise.\n",
        "\n",
        "Finally, let's evaluate the model on the test data. First, we will need to tokenize the test data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Khn2KIijKha3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_dir = os.path.join(imdb_dir, 'test')\n",
        "\n",
        "labels = []\n",
        "texts = []\n",
        "\n",
        "for label_type in ['neg', 'pos']:\n",
        "    dir_name = os.path.join(test_dir, label_type)\n",
        "    for fname in sorted(os.listdir(dir_name)):\n",
        "        if fname[-4:] == '.txt':\n",
        "            f = open(os.path.join(dir_name, fname))\n",
        "            texts.append(f.read())\n",
        "            f.close()\n",
        "            if label_type == 'neg':\n",
        "                labels.append(0)\n",
        "            else:\n",
        "                labels.append(1)\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "x_test = pad_sequences(sequences, maxlen=maxlen)\n",
        "y_test = np.asarray(labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GjNFMopK868",
        "colab_type": "text"
      },
      "source": [
        "And let's load and evaluate the first model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBEYEeezK3NI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "bc429458-fcfc-4fd9-f2e0-e456dea118d9"
      },
      "source": [
        "model.load_weights('pre_trained_glove_model.h5')\n",
        "model.evaluate(x_test, y_test)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "782/782 [==============================] - 2s 3ms/step - loss: 0.7289 - accuracy: 0.5320\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.728891909122467, 0.5320000052452087]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knzMeGg4LN47",
        "colab_type": "text"
      },
      "source": [
        "We get an appalling test accuracy of 54%. Working with just a handful of training samples is hard!"
      ]
    }
  ]
}