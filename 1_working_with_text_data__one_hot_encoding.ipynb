{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1_working_with_text_data__one_hot_encoding.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMSur7QVjzhBwNK8i9QNlT4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/deep-learning-with-python-francois-chollet/blob/6-deep-learning-for-text-and-sequences/1_working_with_text_data__one_hot_encoding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCLcK4vPkXXT",
        "colab_type": "text"
      },
      "source": [
        "# Working with text data: One-hot encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhQlXAUmkwsv",
        "colab_type": "text"
      },
      "source": [
        "Text is one of the most widespread forms of sequence data. It can be understood as either a sequence of characters or a sequence of words, but it’s most common to work at the level of words.\n",
        "\n",
        "The deep-learning sequence-processing models can use text to produce a basic form of natural-language understanding, sufficient for applications including:\n",
        "\n",
        "* document classification, \n",
        "* sentiment analysis, \n",
        "* author identification, \n",
        "* and even question-answering (QA)\n",
        "\n",
        "Of course, keep in mind that none of these deeplearning models truly understand text in a human sense; rather, these models can map the statistical structure of written language, which is sufficient to solve many simple textual tasks. \n",
        "\n",
        "Deep learning for natural-language processing is pattern recognition\n",
        "applied to words, sentences, and paragraphs, in much the same way that computer vision is pattern recognition applied to pixels.\n",
        "\n",
        "Like all other neural networks, deep-learning models don’t take as input raw text: they only work with numeric tensors. **Vectorizing text** is the process of transforming text into numeric tensors. \n",
        "\n",
        "This can be done in multiple ways:\n",
        "* Segment text into words, and transform each word into a vector.\n",
        "* Segment text into characters, and transform each character into a vector.\n",
        "* Extract n-grams of words or characters, and transform each n-gram into a vector.N-grams are overlapping groups of multiple consecutive words or characters.\n",
        "\n",
        "Collectively, the different units into which you can break down text (words, characters, or n-grams) are called tokens, and breaking text into such tokens is called tokenization. All text-vectorization processes consist of applying some tokenization scheme and then associating numeric vectors with the generated tokens. These vectors, packed into sequence tensors, are fed into deep neural networks. \n",
        "\n",
        "There are multiple ways to associate a vector with a token. I’ll present two major ones: \n",
        "\n",
        "* one-hot encoding of tokens, \n",
        "* and token embedding (typically used exclusively for words, and called\n",
        "word embedding).\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/deep-learning-with-python/text-to-tokens-to-vectors.png?raw=1' width='800'/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UX6W7ry3oZ0i",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-nh6BM2ocDO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import string\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Trg_rlvaogpI",
        "colab_type": "text"
      },
      "source": [
        "## One-hot encoding of words and characters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAm_0dVAoi_m",
        "colab_type": "text"
      },
      "source": [
        "One-hot encoding is the most common, most basic way to turn a token into a vector. It consists in associating a unique integer index to every word, then turning this integer index i into a binary vector of size N, the size of the vocabulary, that would be all-zeros except for the i-th entry, which would be 1.\n",
        "\n",
        "Of course, one-hot encoding can be done at the character level as well. To unambiguously drive home what one-hot encoding is and how to implement it, here are two toy examples of one-hot encoding: one for words, the other for characters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FHsRU8zte-N",
        "colab_type": "text"
      },
      "source": [
        "### Word level one-hot encoding (toy example):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItjbXps3pMgf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This is our initial data; one entry per \"sample\" (in this toy example, a \"sample\" is just a sentence, but it could be an entire document).\n",
        "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
        "\n",
        "#  First, build an index of all tokens in the data.\n",
        "token_index = {}\n",
        "for sample in samples:\n",
        "  '''\n",
        "  We simply tokenize the samples via the `split` method.\n",
        "  in real life, we would also strip punctuation and special characters from the samples.\n",
        "  '''\n",
        "  for word in sample.split():\n",
        "    if word not in token_index:\n",
        "      # Assign a unique index to each unique word, Note that we don't attribute index 0 to anything.\n",
        "      token_index[word] = len(token_index) + 1\n",
        "\n",
        "'''\n",
        "Next, we vectorize our samples.\n",
        "We will only consider the first `max_length` words in each sample.\n",
        "'''\n",
        "max_length = 10\n",
        "\n",
        "# This is where we store our results:\n",
        "results = np.zeros((len(samples), max_length, max(token_index.values()) + 1))\n",
        "for i, sample in enumerate(samples):\n",
        "  for j, word in list(enumerate(sample.split()))[:max_length]:\n",
        "    index = token_index.get(word)\n",
        "    results[i, j, index] = 1."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIwcfo3ytBNL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "outputId": "c9528733-0526-4bdd-e2c9-68f1b0ee4685"
      },
      "source": [
        "results"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8LjZEp0thyB",
        "colab_type": "text"
      },
      "source": [
        "### Character-level one-hot encoding (toy example)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNtan1i6tRTW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
        "\n",
        "characters = string.printable  # All printable ASCII characters.\n",
        "\n",
        "#  First, build an index of all tokens in the data.\n",
        "token_index = dict(zip(characters, range(1, len(characters) + 1)))\n",
        "\n",
        "'''\n",
        "Next, we vectorize our samples.\n",
        "We will only consider the first `max_length` words in each sample.\n",
        "'''\n",
        "max_length = 50\n",
        "\n",
        "results = np.zeros((len(samples), max_length, max(token_index.values()) + 1))\n",
        "for i, sample in enumerate(samples):\n",
        "  for j, character  in list(enumerate(sample.split()))[:max_length]:\n",
        "    index = token_index.get(character)\n",
        "    results[i, j, index] = 1."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whSV1MS_ulw4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "outputId": "f9c73f81-21a3-4593-a33e-958b5eee103d"
      },
      "source": [
        "results"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[1., 1., 1., ..., 1., 1., 1.],\n",
              "        [1., 1., 1., ..., 1., 1., 1.],\n",
              "        [1., 1., 1., ..., 1., 1., 1.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       [[1., 1., 1., ..., 1., 1., 1.],\n",
              "        [1., 1., 1., ..., 1., 1., 1.],\n",
              "        [1., 1., 1., ..., 1., 1., 1.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlUcAKr3u2eQ",
        "colab_type": "text"
      },
      "source": [
        "## One-hot encoding using Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fE9uC8k-u70T",
        "colab_type": "text"
      },
      "source": [
        "Note that Keras has built-in utilities for doing one-hot encoding text at the word level or character level, starting from raw text data. This is what you should actually be using, as it will take care of a number of important features, such as stripping special characters from strings, or only taking into the top N most common words in your dataset (a common restriction to avoid dealing with very large input vector spaces)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWYRzbpVu_mW",
        "colab_type": "text"
      },
      "source": [
        "Using Keras for word-level one-hot encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulGIQSCFunFz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}