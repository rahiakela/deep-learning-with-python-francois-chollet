{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "classifying-newswires--multiclass-classification-example.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/deep-learning-with-python-francois-chollet/blob/3-getting-started-with-neural-networks/classifying_newswires_multiclass_classification_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKwGCsNQ3btw",
        "colab_type": "text"
      },
      "source": [
        "# Classifying newswires: a multiclass classification example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rALocsIe3d5O",
        "colab_type": "text"
      },
      "source": [
        "In this guide, you’ll build a network to classify Reuters newswires into 46 mutually\n",
        "exclusive topics. Because you have many classes, this problem is an instance of multiclass\n",
        "classification; and because each data point should be classified into only one category,\n",
        "the problem is more specifically an instance of single-label, multiclass classification.\n",
        "If each data point could belong to multiple categories (in this case, topics), you’d be\n",
        "facing a multilabel, multiclass classification problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOcX0zSg37kH",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVg2aKsC39NC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "552b22b3-0f0c-43c0-c46d-f51335dd42a3"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKjuR2ME3kpZ",
        "colab_type": "text"
      },
      "source": [
        "## The Reuters dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMusJQCo3lrs",
        "colab_type": "text"
      },
      "source": [
        "You’ll work with the Reuters dataset, a set of short newswires and their topics, published\n",
        "by Reuters in 1986. It’s a simple, widely used toy dataset for text classification. There\n",
        "are 46 different topics; some topics are more represented than others, but each topic\n",
        "has at least 10 examples in the training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jz8AARfO4V6H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "f6c1015b-dc38-4066-de66-6d660c3806cc"
      },
      "source": [
        "from keras.datasets import reuters\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/reuters.npz\n",
            "2113536/2110848 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4ZFD3nD45Fu",
        "colab_type": "text"
      },
      "source": [
        "As with the IMDB dataset, the argument num_words=10000 restricts the data to the\n",
        "10,000 most frequently occurring words found in the data.\n",
        "\n",
        "You have 8,982 training examples and 2,246 test examples:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCCPVNFQ4zBX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "dd176c1e-f4bd-456d-fca5-5fa156191de7"
      },
      "source": [
        "len(train_data)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8982"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLZSO6th4-uV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4588f3d0-de98-44da-f4f0-603571d4f9e7"
      },
      "source": [
        "len(test_data)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2246"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1W1qUFyw5G8M",
        "colab_type": "text"
      },
      "source": [
        "As with the IMDB reviews, each example is a list of integers (word indices):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4ligJzZ5CPW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6ace5561-6690-4717-da4a-e1543c94a651"
      },
      "source": [
        "print(train_data[10])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 245, 273, 207, 156, 53, 74, 160, 26, 14, 46, 296, 26, 39, 74, 2979, 3554, 14, 46, 4689, 4329, 86, 61, 3499, 4795, 14, 61, 451, 4329, 17, 12]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thDcrWwb5b7E",
        "colab_type": "text"
      },
      "source": [
        "Here’s how you can decode it back to words, in case you’re curious."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGQf5An95KB8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "0a7a274e-1b4e-485d-f748-caff79b7190d"
      },
      "source": [
        "word_index = reuters.get_word_index()\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "# Note that our indices were offset by 3\n",
        "# because 0, 1 and 2 are reserved indices for \"padding\", \"start of sequence\", and \"unknown\".\n",
        "decoded_newswire = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])\n",
        "decoded_newswire"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'? ? ? said as a result of its december acquisition of space co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and rental operation revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs reuter 3'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fc1KAwE6NpL",
        "colab_type": "text"
      },
      "source": [
        "The label associated with an example is an integer between 0 and 45: a topic index."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfPFhpMq6ELh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3e27db5a-8d0b-4339-f0b2-a471dd3f90e9"
      },
      "source": [
        "train_labels[10]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQFBU7Jw6dDn",
        "colab_type": "text"
      },
      "source": [
        "## Preparing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dBbFr7p6eBK",
        "colab_type": "text"
      },
      "source": [
        "You can vectorize the data with the exact same code as in the previous example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zsHNZJ-6TVM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def vectorize_sequences(sequences, dimension=10000):\n",
        "  results = np.zeros((len(sequences), dimension))\n",
        "  for i, sequence in enumerate(sequences):\n",
        "    results[i, sequence] = 1.\n",
        "  return results\n",
        "\n",
        "# Our vectorized training data\n",
        "x_train = vectorize_sequences(train_data)   \n",
        "# Our vectorized test data\n",
        "x_test = vectorize_sequences(test_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVwrrrS08MkQ",
        "colab_type": "text"
      },
      "source": [
        "To vectorize the labels, there are two possibilities: you can cast the label list as an integer\n",
        "tensor, or you can use one-hot encoding. One-hot encoding is a widely used format\n",
        "for categorical data, also called categorical encoding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Z2gFGsc8Ffz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def to_one_hot(labels, dimension=46):\n",
        "  results = np.zeros((len(labels), dimension))\n",
        "  for i, label in enumerate(labels):\n",
        "    results[i, label] = 1.\n",
        "\n",
        "# Our vectorized training labels\n",
        "one_hot_train_labels = to_one_hot(train_labels)\n",
        "# Our vectorized test labels\n",
        "one_hot_test_labels = to_one_hot(test_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oz7x6DuV9F8Z",
        "colab_type": "text"
      },
      "source": [
        "Note that there is a built-in way to do this in Keras, which you have already seen in action in our MNIST example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiCUGFo_9FU1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "one_hot_train_labels = to_categorical(train_labels)\n",
        "one_hot_test_labels = to_categorical(test_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_P8g_ct9egl",
        "colab_type": "text"
      },
      "source": [
        "## Building your network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95cw31Tr9kKj",
        "colab_type": "text"
      },
      "source": [
        "This topic-classification problem looks similar to the previous movie-review classification\n",
        "problem: in both cases, you’re trying to classify short snippets of text. But there is\n",
        "a new constraint here: the number of output classes has gone from 2 to 46. The\n",
        "dimensionality of the output space is much larger.\n",
        "\n",
        "In a stack of Dense layers like that you’ve been using, each layer can only access information\n",
        "present in the output of the previous layer. \n",
        "\n",
        "If one layer drops some information relevant to the classification problem, this information can never be recovered by later\n",
        "layers: each layer can potentially become an information bottleneck.\n",
        "\n",
        "In the previous\n",
        "example, you used 16-dimensional intermediate layers, but a 16-dimensional space may\n",
        "be too limited to learn to separate 46 different classes: such small layers may act as information\n",
        "bottlenecks, permanently dropping relevant information.\n",
        "\n",
        "For this reason you’ll use larger layers. Let’s go with 64 units."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RonWsFh-Rai",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense \n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(64, activation='relu', input_shape=(10000, )))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(46, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgQ1bmKk_TxA",
        "colab_type": "text"
      },
      "source": [
        "There are two other things you should note about this architecture:\n",
        "\n",
        "* We are ending the network with a Dense layer of size 46. This means that for each input sample, our network will output a 46-dimensional vector. Each entry in this vector (each dimension) will encode a different output class.\n",
        "* The last layer uses a softmax activation. You have already seen this pattern in the MNIST example. It means that the network will output a probability distribution over the 46 different output classes, i.e. for every input sample, the network will produce a 46-dimensional output vector where output[i] is the probability that the sample belongs to class i. The 46 scores will sum to 1.\n",
        "\n",
        "The best loss function to use in this case is categorical_crossentropy. It measures\n",
        "the distance between two probability distributions: here, between the probability distribution\n",
        "output by the network and the true distribution of the labels. By minimizing\n",
        "the distance between these two distributions, you train the network to output something\n",
        "as close as possible to the true labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89CFwybQ-rV8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compiling the model\n",
        "model.compile(optimizer='RMSprop', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OqKSHKSAD-C",
        "colab_type": "text"
      },
      "source": [
        "## Validating our approach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eerd_pwoAFPs",
        "colab_type": "text"
      },
      "source": [
        "Let’s set apart 1,000 samples in the training data to use as a validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fai2ARAb_4gC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}